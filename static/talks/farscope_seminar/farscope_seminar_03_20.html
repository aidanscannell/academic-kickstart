<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Handling Uncertainty in Learnt Probabilistic Transition Dynamics Models</title>
<meta name="author" content="(Aidan Scannell)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://revealjs.com/css/reveal.css"/>

<link rel="stylesheet" href="https://revealjs.com/css/theme/black.css" id="theme"/>

<link rel="stylesheet" href="./custom.css"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://revealjs.com/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section>
<section id="slide-org3821617">
<h2 id="org3821617">Handling Uncertainty in Learnt Probabilistic Transition Dynamics Models</h2>
<p>
Aidan Scannell | Carl Henrik Ek | Arthur Richards
</p>

<p>
5th March 2020
</p>

<p>
aidan.scannell@bristol.ac.uk
</p>
</section>
</section>
<section>
<section id="slide-org5acefe5">
<h2 id="org5acefe5">Uncertainty in Machine Learning</h2>
<div class="outline-text-2" id="text-org5acefe5">
</div>
</section>
<section id="slide-orgb02a7c7">
<h3 id="orgb02a7c7">Incomplete Coverage of the Domain (Epistemic Uncertainty)</h3>

<div class="figure">
<p><img src="images/limited_data2.png" alt="limited_data2.png" />
</p>
</div>

</section>
<section id="slide-org0eb068d">
<h3 id="org0eb068d">Noise in Data (Aleatoric Uncertainty)</h3>
<div class="org-src-container">

<pre  class="src src-python"><code trim>
</code></pre>
</div>



<div class="figure">
<p><img src="images/aleatoric.png" alt="aleatoric.png" />
</p>
</div>

</section>
<section id="slide-org6ca2fdd">
<h3 id="org6ca2fdd">Imperfect Models</h3>

<div class="figure">
<p><img src="images/imperfect_models.png" alt="imperfect_models.png" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-orga6abd17">
<h2 id="orga6abd17">Bayesian Machine Learning</h2>
<ul>
<li>Goal is to infer parameters \(\theta\\\) from data,</li>
<li>Then make predicitions using our learned model.</li>

</ul>

<p>
<i>Predictions vary depending on the type of task (classification, regression, clustering, etc)</i>
</p>
<aside class="notes">
<p>
Understanding how we make predicitions is key to understanding why we care about modelling a distribution over the model parameters.
</p>

</aside>


</section>
<section id="slide-org3a1578e">
<h3 id="org3a1578e">Baye's Rule</h3>
<ul>
<li>Set of observed data \((\mathcal{D} = \{\mathbf{x}, \mathbf{y}\})\) (supervised learning),</li>
<li>Parameterized a model with parameters \((\pmb\theta)\),</li>
<li>We wish to obtain the posterior over the parameters,</li>

</ul>
<p>
\[p(\mathbf{\theta}|\mathcal{D}) = \frac{p(\mathcal{D}|\mathbf{\theta})p(\mathbf{\theta})}{p(\mathcal{D})},\]
</p>

</section>
<section id="slide-org59ce405">
<h3 id="org59ce405"></h3>
<p>
so that we can make predicitions,
\[
p(\mathbf{y}_*| \mathbf{x}_*, \mathcal{D}) = \int p(\mathbf{y}_* | \mathbf{x}_*, \theta, \mathcal{D}) p(\theta | \mathcal{D}) \text{d} \theta,
\]
where \((\mathbf{x}_*\)) is a previously unseen test input and \((\mathbf{y}_*)\) is its corresponding output value.
</p>

</section>
<section id="slide-org89df1df">
<h3 id="org89df1df"></h3>
<p>
This is very cool when you think about it!
</p>
</section>
</section>
<section>
<section id="slide-orgdf4a7f8">
<h2 id="orgdf4a7f8">Gaussian Processes</h2>
<p>
<b><b>Definition</b></b>: A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
</p>

</section>
<section id="slide-org6eb5208">
<h3 id="org6eb5208">Gaussian Processes</h3>
<p>
A Gaussian process is completely specified by its mean function \(m(\mathbf{x})\) and its covariance function \(k(\mathbf{x}, \mathbf{x}')\),
</p>

<div>
\begin{align}
	m(\mathbf{x}) &= \mathbb{E}[f(\mathbf{x})], \\
	k(\mathbf{x}, \mathbf{x}') &= \mathbb{E}[(f(\mathbf{x}) - m(\mathbf{x}))(f(\mathbf{x}') - m(\mathbf{x}'))].
\end{align}

</div>

<p>
and we can write the Gaussian process as,
</p>

<p>
\[f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')).\]
</p>

</section>
<section id="slide-org10008c9">
<h3 id="org10008c9">Prior</h3>

<div class="figure">
<p><img src="images/gp_prior.png" alt="gp_prior.png" />
</p>
</div>

</section>
<section id="slide-org4a551a7">
<h3 id="org4a551a7">Add Data</h3>

<div class="figure">
<p><img src="images/gp_prior_and_data.png" alt="gp_prior_and_data.png" />
</p>
</div>


</section>
<section id="slide-org65485fd">
<h3 id="org65485fd">Condition GP Prior on Data</h3>

<div class="figure">
<p><img src="images/gp_post_samples.png" alt="gp_post_samples.png" />
</p>
</div>

</section>
<section id="slide-org6e3effe">
<h3 id="org6e3effe">GP Posterior</h3>

<div class="figure">
<p><img src="images/gp_post_mu_var.png" alt="gp_post_mu_var.png" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-org7c5f825">
<h2 id="org7c5f825">Background</h2>
<p>
Consider dynamical systems,
</p>
<div>
\begin{align*}
    \mathbf{s}_t &= f(\mathbf{s}_{t-1}, \mathbf{a}_{t-1})
    \DeclareMathOperator{\E}{\mathbb{E}}
    \DeclareMathOperator{\R}{\mathbb{R}}
\end{align*}

</div>

<ul>
<li>State \(\mathbf{s} \in \R^D\)</li>
<li>Action \(\mathbf{a} \in \R^F\)</li>
<li>Time \(t\)</li>
<li>Transition dynamics \(f\)</li>

</ul>

</section>
<section id="slide-orgf654dea">
<h3 id="orgf654dea"></h3>
<p>
where,
</p>

<div>
\begin{equation*}
    f = \begin{cases}
      f_1 + \epsilon_1 \\
      f_2 + \epsilon_2 \\
    \end{cases}
\end{equation*}

</div>
<div>
\begin{equation*}
    \epsilon_i \sim \mathcal{N}(0, \Sigma_{i})\\ 
    \epsilon_1 \gg \epsilon_2
\end{equation*}

</div>

</section>
<section id="slide-orge94759b">
<h3 id="orge94759b"></h3>
<div>
\begin{equation*}
    \Delta x = f(x, y) 
\end{equation*}

</div>
<img style="background:none; border:none; box-shadow:none;" src="images/trajectory.png" width="50%"/>

</section>
<section id="slide-orgb181e56">
<h3 id="orgb181e56"></h3>
<img style="background:none; border:none; box-shadow:none;" src="images/quiver.png" width="70%"/>

</section>
</section>
<section>
<section id="slide-orgedd1590">
<h2 id="orgedd1590">Model</h2>
<img style="background:none; border:none; box-shadow:none;" src="images/graphical-model.png"/>

</section>
<section id="slide-org6415512">
<h3 id="org6415512">Probability Time</h3>
<ul>
<li data-fragment-index="1" class="fragment roll-in">\begin{equation}
  p(\mathbf{Y} | \mathbf{F}, \pmb{\alpha}) = {\displaystyle &prod;_{n=1}^{N}} \mathcal{N} (\mathbf{y}_n|\mathbf{f}_n^{(1)}, &epsilon;_1)^{&alpha;_n} \mathcal{N} (\mathbf{y}_n|\mathbf{f}_n^{(2)} &epsilon;_2)^{1 - &alpha;_n},
\end{equation}</li>

<li data-fragment-index="2" class="fragment roll-in">\begin{equation}
  p(\mathbf{F} | \mathbf{X}) = &prod;^K_{k=1} \mathcal{N}(\mathbf{F}^{(k)}|\mathbf{0}, k^{(k)}({\mathbf{X},\mathbf{X}})),
\end{equation}</li>

<li data-fragment-index="3" class="fragment roll-in">\begin{equation}
  p(h | \mathbf{X}) &sim; \mathcal{N}(h | &mu;_h(\mathbf{X}), k_h(\mathbf{X}, \mathbf{X}))
\end{equation}</li>

</ul>

</section>
</section>
<section>
<section id="slide-org91ea9af">
<h2 id="org91ea9af">Variational Approximation</h2>
<img style="background:none; border:none; box-shadow:none;" src="images/augmented-graphical-model.png"/>

</section>
<section id="slide-org4c8b472">
<h3 id="org4c8b472">Maths</h3>
<p>
As seen in \cite{Hensman}, for each GP we introduce a set of pseudo "samples" from the same prior,
</p>
<div>
\begin{align}
p(\mathbf{u}^{(k)} | \mathbf{Z}^{(k)}) &= \prod^F_{j=1} \mathcal{N}(\mathbf{u}_{:,j}^{(k)} | \mathbf{0}, k^{(k)}(\mathbf{Z}^{(k)}, \mathbf{Z}^{(k)})), \\
p(\mathbf{u}_h | \mathbf{Z}_h) &= \mathcal{N}(\mathbf{u}_h | \mathbf{\mu}_h, k_h(\mathbf{Z}_h, \mathbf{Z}_h)),
\end{align}

</div>

</section>
<section id="slide-org4f0cd3c">
<h3 id="org4f0cd3c"></h3>
<p>
The resulting augmented joint probability distribution takes the form,
</p>
<div>
\begin{equation}
\begin{split}
	p(\mathbf{Y}, \mathbf{F}, \pmb{\alpha}, \mathbf{H}, \mathbf{U} | \mathbf{X}, \mathbf{Z}) = &\ p(\pmb{\alpha}|\mathbf{h}) p(\mathbf{h} | \mathbf{u}_h, \mathbf{X}, \mathbf{Z}_h) p(\mathbf{u}_h | \mathbf{Z}_h) \\ 
  \prod^K_{k=1} \prod^F_{j=1} p(\mathbf{y}_{:,j} | &\mathbf{f}^{(k)}_{:,j}, \pmb{\alpha}) p(\mathbf{f}^{(k)}_{:,j} | \mathbf{u}_{:,j}^{(k)}, \mathbf{X})  p(\mathbf{u}_{:,j}^{(k)} | \mathbf{Z}^{(k)}) 
\end{split}
\end{equation}

</div>

</section>
<section id="slide-orgbf77d60">
<h3 id="orgbf77d60"></h3>
<p>
The variational posteriors of our dynamics \(\mathbf{F}\) and separation manifold \(\mathbf{H}\) take the form,
</p>
<div>
\begin{align}
	q(\mathbf{F}^{(k)} | \mathbf{X}) &= \int q(\mathbf{U}^{(k)}) \prod^N_{n=1} p(\mathbf{f}^{(k)}_n | \mathbf{U}^{(k)}, \mathbf{x}_n) \text{d} \mathbf{U}^{(k)}, \\
	q(\mathbf{H} | \mathbf{X}) &= \int q(\mathbf{U}_h) \prod^N_{n=1} p(\mathbf{h}_n | \mathbf{U}_h, \mathbf{x}_n) \text{d} \mathbf{U}_h.
\end{align}

</div>

</section>
<section id="slide-org497891a">
<h3 id="org497891a"></h3>
<p>
Our variational posterior takes the factorized form,
</p>
<div>
\begin{equation}
	q(\mathbf{H}, \mathbf{F}, \mathbf{U}) = \prod^K_{k=1} \displaystyle\prod_{n=1}^N p(\mathbf{h}_n | \mathbf{U}_h, \mathbf{x}_n) q(\mathbf{U}_h) p(\mathbf{f}^{(k)}_n | \mathbf{U}^{(k)}, \mathbf{x}_n) q(\mathbf{U}^{(k)}).
\end{equation}

</div>

</section>
<section id="slide-orgc50d011">
<h3 id="orgc50d011">Lower Bound</h3>
<div>
\begin{align}
	\mathcal{L} &= \sum_{n=1}^N \E_{q(\mathbf{h}_n)}\bigg[\text{log}\ p(\mathbf{y}_n | \mathbf{f}_n, \pmb{\alpha}_n) p(\pmb{\alpha}_n | \mathbf{h}_n)  \bigg] \\
	&+ \sum_{n=1}^N \E_{q(\mathbf{f}_n)}\bigg[\text{log}\ p(\mathbf{y}_n | \mathbf{f}_n, \pmb{\alpha}_n) p(\pmb{\alpha}_n | \mathbf{h}_n)  \bigg] \\
	&\ - \text{KL}(q(\mathbf{U}_h) || p(\mathbf{U}_h)) \\
	&\ - \sum^K_{k=1} \text{KL}(q(\mathbf{U}^{(k)}) || p(\mathbf{U}^{(k)})).
\end{align}

</div>

</section>
</section>
<section>
<section id="slide-orgc8a42af">
<h2 id="orgc8a42af">Results</h2>
<p class="fragment fade-in-then-out">Remember $$\Delta x = f(x, y)$$</p>

<p class="fragment fade-in"><img style="background:none; border:none; box-shadow:none;" src="images/member-berries.jpg" width="500px"/></p>

</section>
<section id="slide-orgb9a2d1a">
<h3 id="orgb9a2d1a">\(\Delta x\)</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/y_dim_1.png" height="80%" width="100%"/>

</section>
<section id="slide-org42ead26">
<h3 id="org42ead26">\(f_1\)</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/f1_dim_1.png" width="1900px"/>

</section>
<section id="slide-org0c6715f">
<h3 id="org0c6715f">\(f_2\)</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/f2_dim_1.png" width="1900px"/>

</section>
<section id="slide-org054ea3c">
<h3 id="org054ea3c">\(h\)</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/h.png" width="1900px"/>

</section>
<section id="slide-org74e225c">
<h3 id="org74e225c">\(\alpha\)</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/alpha.png" width="1900px"/>

</section>
</section>
<section>
<section id="slide-org6baf5a7">
<h2 id="org6baf5a7">Ok Great, But Why???</h2>
</section>
</section>
<section>
<section id="slide-org56b9bc2">
<h2 id="org56b9bc2">Trajectory Optimisation</h2>
<p>
We would like to find a trajectory (curve) in \(\mathbf{X} \in \R^2\) that,
</p>

<ol>
<li>Connects two points,</li>
<li>Minimises distance,</li>
<li>Avoids regions with high aleatoric uncertainty (\(\alpha > 0.5\)),</li>
<li>Avoids regions with high epistemic uncertainty,</li>

</ol>

</section>
<section id="slide-org0d46fb2">
<h3 id="org0d46fb2">What?</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/dx_quiver.png" width="500px"/>

</section>
<section id="slide-org8acbd3c">
<h3 id="org8acbd3c">Lets Use Our Model!</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/h.png" height="80%" width="100%"/>
<img style="background:none; border:none; box-shadow:none;" src="images/f1_dim_1.png" height="80%" width="100%"/>

</section>
</section>
<section>
<section id="slide-orge2f3be7">
<h2 id="orge2f3be7">Geodesics</h2>
<p>
<b>Geodesic</b>: Given two points \(\mathbf{x}_1, \mathbf{x}_2 \in
\mathcal{M}\), a Geodesic is a length minimising curve \(\mathbf{c}_g\) connecting the points such
that,
</p>
<div>
\begin{align}
  \mathbf{c}_{g}=\arg \min _{\mathbf{c}} \operatorname{Length}(\mathbf{c}), \quad \mathbf{c}(0)=\mathbf{x}_{1}, \mathbf{c}(1)=\mathbf{x}_{2}.
\end{align}

</div>

</section>
<section id="slide-org8fa2654">
<h3 id="org8fa2654">How do we Calculate Lengths on Manifolds??</h3>

</section>
<section id="slide-org998c4a6">
<h3 id="org998c4a6">Riemmanian Metric</h3>
<p>
<b>Riemannian Metric</b>: A Riemannian metric \(\mathbf{G}\) on a
manifold \(\mathcal{M}\) is a symmetric and positive definite matrix which defines
a smoothly varying inner product,
</p>
<div>
\begin{align}
  \langle \mathbf{a}, \mathbf{b} \rangle_x = \mathbf{a}^T \mathbf{G}(x) \mathbf{b}
\end{align}

</div>
<p>
in the tangent space \(T_x\mathcal{M}\), for each point \(x \in \mathcal{M}\) and
\(\mathbf{a}, \mathbf{b} \in T_x\mathcal{M}\). The matrix \(\mathbf{G}\) is called
the metric tensor.
</p>

</section>
<section id="slide-org7f3730c">
<h3 id="org7f3730c">Lets Imagine a Random Manifold</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/original_gp_mean.png" height="80%" width="100%"/>
</section>
<section id="slide-org9d6631d">
<h3 id="org9d6631d">Lets Visualise G(x) for a Random Manifold</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/gradient_mean_quiver_just_mean.png" height="80%" width="100%"/>
</section>
<section id="slide-org281d7fa">
<h3 id="org281d7fa">Lets Visualise G(x)</h3>
<img style="background:none; border:none; box-shadow:none;" src="images/original_gp_mean.png" height="80%" width="100%"/>
<img style="background:none; border:none; box-shadow:none;" src="images/gradient_mean.png" height="80%" width="100%"/>

</section>
<section id="slide-orgabbc68b">
<h3 id="orgabbc68b">Lengths on Manifolds</h3>
<p>
On a Riemannian manidold \(\mathcal{M}\), the length of a curce \(\mathbf{c} : [0, 1]
\rightarrow \mathcal{M}\) is given by the norm of the tangent vector (velocity)
along the curve,
</p>
<div>
\begin{align}\label{eq:length}
  \text { Length }(\mathbf{c}) &=\int_{0}^{1}\left\|\mathbf{c}^{\prime}(\lambda)\right\|_{\mathbf{G}(\mathbf{c}(\lambda))} \mathrm{d} \lambda \\
  &=\int_{0}^{1} \sqrt{\mathbf{c}^{\prime}(\lambda)^{T} \mathbf{G}(\mathbf{c}(\lambda)) \mathbf{c}^{\prime}(\lambda)} \mathrm{d} \lambda
\end{align}

</div>
<p>
where \(\mathbf{c}'\) denotes the derivative of \(\mathbf{c}\) and \(\mathbf{G}(\mathbf{c}(\lambda))\) is the metric tensor at \(\mathbf{c}(\lambda)\).
</p>

</section>
<section id="slide-org489adbc">
<h3 id="org489adbc"></h3>
<p>
It follows that Geodesics satisfy the following second order ODE,
</p>
<div>
\begin{align*}  
\mathbf{c}^{\prime \prime}(\lambda)&=\mathbf{f}\left(\lambda, \mathbf{c}, \mathbf{c}^{\prime}\right)
  \\
  &=-\frac{1}{2} \mathbf{G}^{-1}(\mathbf{c}(\lambda))\left[\frac{\partial \operatorname{vec}[\mathbf{G}(\mathbf{c}(\lambda))]}{\partial \mathbf{c}(\lambda)}\right]^{T}\left(\mathbf{c}^{\prime}(\lambda) \otimes \mathbf{c}^{\prime}(\lambda)\right)
\end{align*}

</div>

</section>
<section id="slide-orgb9121ae">
<h3 id="orgb9121ae"></h3>
<p>
Which can be expressed as a system of 1st order equations.
We let \(\mathbf{g}(\lambda) = \mathbf{c}'(\lambda)\) and express
equation~\ref{eq:2ode} by solving for \(\mathbf{c}\) and \(\mathbf{c}'\),
</p>
<div>
\begin{align}
  \label{eq:1ode}
  \left[\begin{array}{l}
          {\mathbf{c}^{\prime}(\lambda)} \\
          {\mathbf{g}^{\prime}(\lambda)}
        \end{array}\right]=\left[\begin{array}{c}
                                   {\mathbf{g}(\lambda)} \\
                                   {\mathbf{f}(\lambda, \mathbf{c}, \mathbf{g})}
                                 \end{array}\right]
\end{align}

</div>

</section>
</section>
<section>
<section id="slide-orgf1929bb">
<h2 id="orgf1929bb">Probabilistic Geodesics</h2>
<p>
Lets introduce the following Reimannian metric,
</p>
<div>
\begin{align}
  \langle \mathbf{a}, \mathbf{b} \rangle_x = \mathbf{a}^T \mathbf{J}^T \mathbf{J} \mathbf{b} =
  \mathbf{a}^T \mathbf{G}(x) \mathbf{b}
\end{align}

</div>
<p>
where \(\mathbf{J}\) denotes the Jacobian of h,
</p>
<div>
\begin{align}
  [\mathbf{J}]_{j}=\frac{\partial h}{\partial l_{j}} = \bigg[ \frac{\partial h}{\partial x}, \frac{\partial h}{\partial y} \bigg].
\end{align}

</div>

</section>
<section id="slide-org8e282e7">
<h3 id="org8e282e7">Quick Maths</h3>
<ul>
<li>The differential operator is linear so the derivative of a GP is again a GP,</li>
<li>So the Jacobian and the output are jointly Gaussian,</li>

</ul>

<div>
\begin{align}
\left[\begin{array}{c}
        {\mathbf{Y}} \\
        {\frac{\partial \mathbf{y}_{*}}{\partial \mathbf{x}}}
      \end{array}\right] \sim \mathcal{N}\left(\mathbf{0},\left[\begin{array}{cc}
                                                                  {\mathbf{K}_{\mathbf{x}, \mathbf{x}}} & {\partial \mathbf{K}_{\mathbf{x}, *}} \\
                                                                  {\partial \mathbf{K}_{\mathbf{x}, *}^{\top}} & {\partial^{2} \mathbf{K}_{*, *}}
                                                                \end{array}\right]\right).
\end{align}

</div>

</section>
<section id="slide-org87dc10f">
<h3 id="org87dc10f"></h3>
<p>
This means that we can easily obtain the conditional distribution \(p(\mathbf{J} | \mathbf{X}, \mathbf{Y}, \mathbf{x}_*)\),
</p>
<div>
\begin{align}
  p(\mathbf{J} | \mathbf{Y}, \mathbf{X}, \mathbf{x}_*) &= \prod^p_{j=1} (\pmb{\mu}_{J(j,:)}, \mathbf{\Sigma}_J), \\
  \underset{D \times 1}{\pmb{\mu}_{J(j,:)}} &= \underset{D \times N}{\partial\mathbf{K}^T_{x,*}} \underset{N \times N}{\mathbf{K}^{-1}_{x,x}} \underset{N \times 1}{\mathbf{Y}_{:,j}},  \\
  \underset{D \times D}{\mathbf{\Sigma}_J} &= \underset{D \times D}{\partial^2\mathbf{K}_{*,*}} - \underset{D \times N}{\partial\mathbf{K}_{x,*}^T} \underset{N \times N}{\mathbf{K}^{-1}_{x,x}} \underset{N \times D}{\partial \mathbf{K}_{x,*}}.
\end{align}

</div>

</section>
<section id="slide-orgd4501a9">
<h3 id="orgd4501a9"></h3>
<p>
Suppose we draw \(n\) samples from this $D-$dimensional normal distribution to get
a matrix \(\mathbf{J}_* \in \R^{D \times n}\).
This induces a non-central Wishart distribution over the metric tensor \(\mathbf{G}\),
</p>
<div>
\begin{align}
  \mathbf{G}=\mathcal{W}_{q}\left(p, \boldsymbol{\Sigma}_{J}, \mathbb{E}\left[\mathbf{J}^{\top}\right] \mathbb{E}[\mathbf{J}]\right),
\end{align}

</div>
<p>
as the Wishart distribution is the probability dist of the \(D \times D\) random matrix
\(\mathbf{G}_* = \mathbf{J}_* \mathbf{J}_*^T\), known as the scatter matrix.
</p>

</section>
<section id="slide-orgc2ecf32">
<h3 id="orgc2ecf32"></h3>
<p>
The expected metric tensor is then given by,
</p>
<div>
\begin{align}
  \E[\mathbf{J}^T \mathbf{J}] = \E[\mathbf{J}^T] \E[\mathbf{J}] + p \mathbf{\Sigma}_J.
\end{align}

</div>
<p>
The expected metric tensor includes a covariance term \(p \mathbf{\Sigma}_J\) which implies that the
metric is larger when the uncertainty in the mapping is higher. This is exactly
what we wanted from our metric tensor!
</p>

</section>
</section>
<section>
<section id="slide-orge3ed37a">
<h2 id="orge3ed37a">Pretty Plots</h2>
<img style="background:none; border:none; box-shadow:none;" src="images/gradient_mean_quiver.png" height="80%" width="100%"/>
<img style="background:none; border:none; box-shadow:none;" src="images/gradient_mean.png" height="80%" width="100%"/>

</section>
<section id="slide-org281df87">
<h3 id="org281df87"></h3>
<img style="background:none; border:none; box-shadow:none;" src="images/gradient_variance_quiver.png" height="80%" width="100%"/>
<img style="background:none; border:none; box-shadow:none;" src="images/gradient_variance.png" height="80%" width="100%"/>
</section>
<section id="slide-orga9ba12e">
<h3 id="orga9ba12e"></h3>
<img style="background:none; border:none; box-shadow:none;" src="images/trace(G(x)).png" height="80%" width="100%"/>
<img style="background:none; border:none; box-shadow:none;" src="images/G(x).png" height="80%" width="100%"/>

</section>
</section>
<section>
<section id="slide-org8c6f4c3">
<h2 id="org8c6f4c3">Results</h2>
<img style="background:none; border:none; box-shadow:none;" src="images/optimised-geodesic.png" width="100%"/>
</section>
</section>
<section>
<section id="slide-orgb3e7397">
<h2 id="orgb3e7397">Thanks for Listening</h2>
</section>
</section>
</div>
</div>
<script src="https://revealjs.com/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://revealjs.com/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://revealjs.com/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://revealjs.com/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://revealjs.com/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://revealjs.com/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
